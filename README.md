# American_Sign_Language_Detection_MNIST
Classification of American Sign Language (ASL) letters into english alphabets using various machine learning methods and finally comparing it with CNN.

Various kinds of sign languages are being used in different countries such as the USA, Argentina, India, Germany,Ireland, Greece, Spain, China, Korea, Iran, etc. However,there is no robust system for communication between the users subscribing to these different sign languages. We aim to build a module to help attain this milestone of designing a robust system for translation between dissimilar sign languages and spoken languages. However, for this final
project’s scope, we are only working with the classification of American Sign Language into English alphabet as shown in Figure 1. We aim to leverage this final project to gain an insight into the complex problem statement mentioned above.
This project will build our foundations to tackle the challenge of designing this robust sign language translation system. In the scope of this project we will be comparing different classical machine learning methods and deep learning methods to analyze detection and classification of American Sign Language alphabets. 
To begin, we will use logistic regression, support vector machines and random forest to classify the samples based on the original data. Additionally,
we would then also use the same classifiers on transformed data using dimensionality reduction techniques such as Princial Component Analysis (PCA), Linear Discriminant Analysis (LDA) and Autoencoders (AE). Further, we will also be using various deep learning methods using inspiration from a popular architecture called ResNet and also our own custom architecture to apply the same classification techniques and compare their results on the original data.

Grayscale images of American Sign Language letters of 28x28 pixels resolution with 0-255 pixel values.


The idea for the Sign Language MNIST dataset comes from the original MNIST dataset for handwritten digits, which is rather simple to work with. It consists of 28x28 images arranged as 784 pixels in csv format along with the labels in single rows. There are gray scale images of values between 0 to 255 representing hand gestures for all letters of the English alphabet except ’J’ and ’Z’. This is because the two letters require gesture motions during communication. There are 277,455 training samples and 7172 test samples.A sample of each letter is shown in Figure 2. The classes in the dataset are fairly balanced with the highest class count of 1294 for letter ’R’ and lowest count of 957 for ’E’. The barplot of class count is shown in figure Figure 3. The dataset is normalized using the mean and standard deviation of the training data.
